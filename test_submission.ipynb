{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92e1fb6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>INDEX</th>\n",
       "      <th>TARGET_FLAG</th>\n",
       "      <th>TARGET_AMT</th>\n",
       "      <th>KIDSDRIV</th>\n",
       "      <th>AGE</th>\n",
       "      <th>HOMEKIDS</th>\n",
       "      <th>YOJ</th>\n",
       "      <th>INCOME</th>\n",
       "      <th>PARENT1</th>\n",
       "      <th>HOME_VAL</th>\n",
       "      <th>MSTATUS</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>JOB</th>\n",
       "      <th>TRAVTIME</th>\n",
       "      <th>CAR_USE</th>\n",
       "      <th>BLUEBOOK</th>\n",
       "      <th>TIF</th>\n",
       "      <th>CAR_TYPE</th>\n",
       "      <th>RED_CAR</th>\n",
       "      <th>OLDCLAIM</th>\n",
       "      <th>CLM_FREQ</th>\n",
       "      <th>REVOKED</th>\n",
       "      <th>MVR_PTS</th>\n",
       "      <th>CAR_AGE</th>\n",
       "      <th>URBANICITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>$67,349</td>\n",
       "      <td>No</td>\n",
       "      <td>$0</td>\n",
       "      <td>z_No</td>\n",
       "      <td>M</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Professional</td>\n",
       "      <td>14</td>\n",
       "      <td>Private</td>\n",
       "      <td>$14,230</td>\n",
       "      <td>11</td>\n",
       "      <td>Minivan</td>\n",
       "      <td>yes</td>\n",
       "      <td>$4,461</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>18.0</td>\n",
       "      <td>Highly Urban/ Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>$91,449</td>\n",
       "      <td>No</td>\n",
       "      <td>$257,252</td>\n",
       "      <td>z_No</td>\n",
       "      <td>M</td>\n",
       "      <td>z_High School</td>\n",
       "      <td>z_Blue Collar</td>\n",
       "      <td>22</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>$14,940</td>\n",
       "      <td>1</td>\n",
       "      <td>Minivan</td>\n",
       "      <td>yes</td>\n",
       "      <td>$0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Highly Urban/ Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>10.0</td>\n",
       "      <td>$16,039</td>\n",
       "      <td>No</td>\n",
       "      <td>$124,191</td>\n",
       "      <td>Yes</td>\n",
       "      <td>z_F</td>\n",
       "      <td>z_High School</td>\n",
       "      <td>Clerical</td>\n",
       "      <td>5</td>\n",
       "      <td>Private</td>\n",
       "      <td>$4,010</td>\n",
       "      <td>4</td>\n",
       "      <td>z_SUV</td>\n",
       "      <td>no</td>\n",
       "      <td>$38,690</td>\n",
       "      <td>2</td>\n",
       "      <td>No</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Highly Urban/ Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>$306,251</td>\n",
       "      <td>Yes</td>\n",
       "      <td>M</td>\n",
       "      <td>&lt;High School</td>\n",
       "      <td>z_Blue Collar</td>\n",
       "      <td>32</td>\n",
       "      <td>Private</td>\n",
       "      <td>$15,440</td>\n",
       "      <td>7</td>\n",
       "      <td>Minivan</td>\n",
       "      <td>yes</td>\n",
       "      <td>$0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Highly Urban/ Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$114,986</td>\n",
       "      <td>No</td>\n",
       "      <td>$243,925</td>\n",
       "      <td>Yes</td>\n",
       "      <td>z_F</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>36</td>\n",
       "      <td>Private</td>\n",
       "      <td>$18,000</td>\n",
       "      <td>1</td>\n",
       "      <td>z_SUV</td>\n",
       "      <td>no</td>\n",
       "      <td>$19,217</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>Highly Urban/ Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2946.0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>$125,301</td>\n",
       "      <td>Yes</td>\n",
       "      <td>$0</td>\n",
       "      <td>z_No</td>\n",
       "      <td>z_F</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>z_Blue Collar</td>\n",
       "      <td>46</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>$17,430</td>\n",
       "      <td>1</td>\n",
       "      <td>Sports Car</td>\n",
       "      <td>no</td>\n",
       "      <td>$0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Highly Urban/ Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$18,755</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>z_F</td>\n",
       "      <td>&lt;High School</td>\n",
       "      <td>z_Blue Collar</td>\n",
       "      <td>33</td>\n",
       "      <td>Private</td>\n",
       "      <td>$8,780</td>\n",
       "      <td>1</td>\n",
       "      <td>z_SUV</td>\n",
       "      <td>no</td>\n",
       "      <td>$0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Highly Urban/ Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>4021.0</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$107,961</td>\n",
       "      <td>No</td>\n",
       "      <td>$333,680</td>\n",
       "      <td>Yes</td>\n",
       "      <td>M</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>z_Blue Collar</td>\n",
       "      <td>44</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>$16,970</td>\n",
       "      <td>1</td>\n",
       "      <td>Van</td>\n",
       "      <td>yes</td>\n",
       "      <td>$2,374</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>10</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Highly Urban/ Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2501.0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>$62,978</td>\n",
       "      <td>No</td>\n",
       "      <td>$0</td>\n",
       "      <td>z_No</td>\n",
       "      <td>z_F</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Clerical</td>\n",
       "      <td>34</td>\n",
       "      <td>Private</td>\n",
       "      <td>$11,200</td>\n",
       "      <td>1</td>\n",
       "      <td>z_SUV</td>\n",
       "      <td>no</td>\n",
       "      <td>$0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Highly Urban/ Urban</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>$106,952</td>\n",
       "      <td>No</td>\n",
       "      <td>$0</td>\n",
       "      <td>z_No</td>\n",
       "      <td>M</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>Professional</td>\n",
       "      <td>48</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>$18,510</td>\n",
       "      <td>7</td>\n",
       "      <td>Van</td>\n",
       "      <td>no</td>\n",
       "      <td>$0</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>z_Highly Rural/ Rural</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   INDEX  TARGET_FLAG  TARGET_AMT  KIDSDRIV   AGE  HOMEKIDS   YOJ    INCOME  \\\n",
       "0      1            0         0.0         0  60.0         0  11.0   $67,349   \n",
       "1      2            0         0.0         0  43.0         0  11.0   $91,449   \n",
       "2      4            0         0.0         0  35.0         1  10.0   $16,039   \n",
       "3      5            0         0.0         0  51.0         0  14.0       NaN   \n",
       "4      6            0         0.0         0  50.0         0   NaN  $114,986   \n",
       "5      7            1      2946.0         0  34.0         1  12.0  $125,301   \n",
       "6      8            0         0.0         0  54.0         0   NaN   $18,755   \n",
       "7     11            1      4021.0         1  37.0         2   NaN  $107,961   \n",
       "8     12            1      2501.0         0  34.0         0  10.0   $62,978   \n",
       "9     13            0         0.0         0  50.0         0   7.0  $106,952   \n",
       "\n",
       "  PARENT1  HOME_VAL MSTATUS  SEX      EDUCATION            JOB  TRAVTIME  \\\n",
       "0      No        $0    z_No    M            PhD   Professional        14   \n",
       "1      No  $257,252    z_No    M  z_High School  z_Blue Collar        22   \n",
       "2      No  $124,191     Yes  z_F  z_High School       Clerical         5   \n",
       "3      No  $306,251     Yes    M   <High School  z_Blue Collar        32   \n",
       "4      No  $243,925     Yes  z_F            PhD         Doctor        36   \n",
       "5     Yes        $0    z_No  z_F      Bachelors  z_Blue Collar        46   \n",
       "6      No       NaN     Yes  z_F   <High School  z_Blue Collar        33   \n",
       "7      No  $333,680     Yes    M      Bachelors  z_Blue Collar        44   \n",
       "8      No        $0    z_No  z_F      Bachelors       Clerical        34   \n",
       "9      No        $0    z_No    M      Bachelors   Professional        48   \n",
       "\n",
       "      CAR_USE BLUEBOOK  TIF    CAR_TYPE RED_CAR OLDCLAIM  CLM_FREQ REVOKED  \\\n",
       "0     Private  $14,230   11     Minivan     yes   $4,461         2      No   \n",
       "1  Commercial  $14,940    1     Minivan     yes       $0         0      No   \n",
       "2     Private   $4,010    4       z_SUV      no  $38,690         2      No   \n",
       "3     Private  $15,440    7     Minivan     yes       $0         0      No   \n",
       "4     Private  $18,000    1       z_SUV      no  $19,217         2     Yes   \n",
       "5  Commercial  $17,430    1  Sports Car      no       $0         0      No   \n",
       "6     Private   $8,780    1       z_SUV      no       $0         0      No   \n",
       "7  Commercial  $16,970    1         Van     yes   $2,374         1     Yes   \n",
       "8     Private  $11,200    1       z_SUV      no       $0         0      No   \n",
       "9  Commercial  $18,510    7         Van      no       $0         0      No   \n",
       "\n",
       "   MVR_PTS  CAR_AGE             URBANICITY  \n",
       "0        3     18.0    Highly Urban/ Urban  \n",
       "1        0      1.0    Highly Urban/ Urban  \n",
       "2        3     10.0    Highly Urban/ Urban  \n",
       "3        0      6.0    Highly Urban/ Urban  \n",
       "4        3     17.0    Highly Urban/ Urban  \n",
       "5        0      7.0    Highly Urban/ Urban  \n",
       "6        0      1.0    Highly Urban/ Urban  \n",
       "7       10      7.0    Highly Urban/ Urban  \n",
       "8        0      1.0    Highly Urban/ Urban  \n",
       "9        1     17.0  z_Highly Rural/ Rural  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "\n",
    "#Read data and have a first look at the data\n",
    "pd.set_option('display.max_columns', None)\n",
    "train = pd.read_csv('train_auto.csv')\n",
    "\n",
    "#Check out a sample of the data\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9b49e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# First thing to note: The target flag displays zeroes and ones\n",
    "# in our samples. To make sure these are the only two values:\n",
    "print(train['TARGET_FLAG'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70a6a636",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# So with two classes this is going to be a classification problem\n",
    "# or an anomaly detection problem (which is also classification in the larger sence)\n",
    "print(train.TARGET_FLAG.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229b3ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 6008, 1: 2153})\n",
      "Case 0: 73.61842911407915 % ,  Case 1:  26.38157088592084 % ,  Unasigned cases:  0.0 %\n"
     ]
    }
   ],
   "source": [
    "# Check the number of occurences of both types, as well as of unasigned cases\n",
    "TARGET_FLAG=train.TARGET_FLAG.astype(int)\n",
    "count_flags = Counter(train.TARGET_FLAG)\n",
    "print(count_flags)\n",
    "print('Case 0:', float(count_flags[0])/float(train.shape[0])*100.,'%',\n",
    "      ',  Case 1: ', float(count_flags[1])/float(train.shape[0])*100.,'%',\n",
    "      ',  Unasigned cases: ',float(train.shape[0]-count_flags[0]-count_flags[1])/float(train.shape[0])*100.,'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bfd5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a fair amount of cases in both classes, and therefore \n",
    "# I think that we can use a classification algorithm.\n",
    "\n",
    "\n",
    "# Before thinking about the features to use lets think a second about \n",
    "# what the target_flag could be. Since it is related to car insurance\n",
    "# the two that come immediately to my mind, are either that it is linked\n",
    "# to the occurence of damage over a period of time or that it is linked to\n",
    "# whether insurance is provided or not. The fact that there is a target amount\n",
    "# suggests that target_flag probably refers to damage occurence.\n",
    "\n",
    "\n",
    "# With this in mind let's now have a look at features. Some of them will need \n",
    "# some transormation, in particular those with $ values:\n",
    "train.INCOME=train.INCOME.str.replace(',','').str.extract('(\\d+)', expand=False)\n",
    "train.HOME_VAL=train.HOME_VAL.str.replace(',','').str.extract('(\\d+)', expand=False)\n",
    "train.BLUEBOOK=train.BLUEBOOK.str.replace(',','').str.extract('(\\d+)', expand=False)\n",
    "train.OLDCLAIM \t=train.OLDCLAIM .str.replace(',','').str.extract('(\\d+)', expand=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e1ae5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6045\n",
      "Counter({0: 4443, 1: 1602})\n"
     ]
    }
   ],
   "source": [
    "# Now lets check for missing values\n",
    "# among the features.\n",
    "# We have several ways to proceed:\n",
    "# We can ignore training examples with\n",
    "# missing values.\n",
    "# We can also try to replace missing values\n",
    "# with the mean of the feature (if possible)\n",
    "\n",
    "\n",
    "train_nomiss=train\n",
    "for column in train:\n",
    "    train_nomiss=train_nomiss[pd.notnull(train_nomiss[column])]\n",
    "    \n",
    "print(train_nomiss.shape[0])\n",
    "print(Counter(train_nomiss.TARGET_FLAG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c229e329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGE  min:  16.0  max:  81.0  mean:  44.628453267162946  median:  45.0\n",
      "HOMEKIDS  min:  0.0  max:  5.0  mean:  0.743424317617866  median:  0.0\n",
      "YOJ  min:  0.0  max:  23.0  mean:  10.494623655913978  median:  11.0\n",
      "INCOME  min:  0.0  max:  367030.0  mean:  58177.01323407775  median:  51624.0\n",
      "HOME_VAL  min:  0.0  max:  885282.0  mean:  150102.07460711332  median:  159152.0\n",
      "TRAVTIME  min:  5.0  max:  142.0  mean:  33.69429280397022  median:  33.0\n",
      "BLUEBOOK  min:  1500.0  max:  65970.0  mean:  15235.60959470637  median:  14080.0\n",
      "TIF  min:  1.0  max:  25.0  mean:  5.36029776674938  median:  4.0\n",
      "OLDCLAIM  min:  0.0  max:  57037.0  mean:  4004.875599669148  median:  0.0\n",
      "CLM_FREQ  min:  0.0  max:  5.0  mean:  0.7841191066997518  median:  0.0\n",
      "MVR_PTS  min:  0.0  max:  13.0  mean:  1.6997518610421836  median:  1.0\n",
      "CAR_AGE  min:  -3.0  max:  28.0  mean:  7.920926385442514  median:  8.0\n",
      "  \n",
      "Numerical features:  12\n",
      "Class features:  11\n"
     ]
    }
   ],
   "source": [
    "# If we remove all examples with at least one\n",
    "# missing feature we are still left with about\n",
    "# 75% of the data. So let's for now proceed in this\n",
    "# manner. We could still see if this number drops\n",
    "# later on, if we remove redundant features for example.\n",
    "\n",
    "\n",
    "# Now lets divide the data into features\n",
    "# that divide the cases into \"categories\"\n",
    "# and into features that take a wider range\n",
    "# of values that we denote as \"numeric\".\n",
    "# For the numeric type, let's have a look at\n",
    "# a few simple statistics and see whether\n",
    "# we have any unrealistic looking values.\n",
    "\n",
    "train_np=train_nomiss.to_numpy()\n",
    "features=train_nomiss.loc[:,'KIDSDRIV':'URBANICITY']\n",
    "\n",
    "\n",
    "num_list=[]\n",
    "cat_list=[]\n",
    "feat_name_list=[]\n",
    "for feat in features:\n",
    "    if features[feat].nunique() >=6 and feat!='JOB' and feat!='EDUCATION' and feat!='CAR_TYPE' :\n",
    "        num_list.append(feat)\n",
    "        feat_name_list.append(feat)\n",
    "        print(feat,' min: ',np.amin(np.array(features[feat]).astype(float)), \n",
    "              ' max: ', np.amax(np.array(features[feat]).astype(float)), \n",
    "              ' mean: ', np.mean(np.array(features[feat]).astype(float)),\n",
    "              ' median: ', np.median(np.array(features[feat]).astype(float)))\n",
    "    else: \n",
    "        cat_list.append(feat)\n",
    "print(\"  \")        \n",
    "features_num=np.array(features.loc[:,num_list].astype(float))\n",
    "print(\"Numerical features: \", features_num.shape[1])\n",
    "\n",
    "features_cat=features.loc[:,cat_list]\n",
    "print(\"Class features: \", features_cat.shape[1])\n",
    "\n",
    "target=np.array(train_nomiss.loc[:,'TARGET_FLAG'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01fa22fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6045, 20)\n"
     ]
    }
   ],
   "source": [
    "# The only value that looks out of the ordinary is the negative CAR_AGE.\n",
    "# Maybe the maximum travtime also seems high, but since I am unaware of\n",
    "# the units and since commercially driven cars could spend a lot of time\n",
    "# on the road this could be an issue.\n",
    "# I could now set the minimum car age to zero, but I suspect that this is\n",
    "# not going to affect the prediction substantially.\n",
    "\n",
    "\n",
    "# Let's start transforming the categorical data with two values.\n",
    "# To these, I will just assign 0 and 1. The order should not be important.\n",
    "\n",
    "features_cat.PARENT1=features_cat.PARENT1.replace('No','0').replace('Yes','1')\n",
    "features_cat.MSTATUS=features_cat.MSTATUS.replace('z_No','0').replace('Yes','1')\n",
    "features_cat.SEX=features_cat.SEX.replace('z_F','0').replace('M','1')\n",
    "features_cat.CAR_USE=features_cat.CAR_USE.replace('Private','0').replace('Commercial','1')\n",
    "features_cat.RED_CAR=features_cat.RED_CAR.replace('no','0').replace('yes','1')\n",
    "features_cat.REVOKED=features_cat.REVOKED.replace('No','0').replace('Yes','1')\n",
    "features_cat.URBANICITY=features_cat.URBANICITY.replace('z_Highly Rural/ Rural','0').replace('Highly Urban/ Urban','1')\n",
    "\n",
    "\n",
    "\n",
    "features_cat_2=np.array(features_cat.loc[:,['KIDSDRIV','PARENT1','MSTATUS','SEX',\n",
    "                                            'CAR_USE','RED_CAR','REVOKED','URBANICITY']]).astype(float)\n",
    "feat_name_list.append(['KIDSDRIV','PARENT1','MSTATUS','SEX',\n",
    "                                            'CAR_USE','RED_CAR','REVOKED','URBANICITY'])\n",
    "\n",
    "features_merge=np.concatenate((features_num, features_cat_2), axis=1)\n",
    "print(features_merge.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15e6c2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAR_TYPE\n",
      "Minivan        0.161670\n",
      "Panel Truck    0.299712\n",
      "Pickup         0.321499\n",
      "Sports Car     0.343137\n",
      "Van            0.250000\n",
      "z_SUV          0.297586\n",
      "Name: TARGET_FLAG, dtype: float64\n",
      "EDUCATION\n",
      "<High School     0.327749\n",
      "Bachelors        0.231034\n",
      "Masters          0.169651\n",
      "PhD              0.172662\n",
      "z_High School    0.339209\n",
      "Name: TARGET_FLAG, dtype: float64\n",
      "JOB\n",
      "Clerical         0.302619\n",
      "Doctor           0.135000\n",
      "Home Maker       0.274793\n",
      "Lawyer           0.189552\n",
      "Manager          0.127086\n",
      "Professional     0.218894\n",
      "Student          0.372439\n",
      "z_Blue Collar    0.348238\n",
      "Name: TARGET_FLAG, dtype: float64\n",
      "(6045, 23)\n"
     ]
    }
   ],
   "source": [
    "# Let's integrate the remainder of the\n",
    "# categorical data. For the multi-category categories, I\n",
    "# will assign each category the percentage of positives. This may\n",
    "# lead to overfitting the data, but I will check anyway if \n",
    "# this is a problem later with the test set.\n",
    "\n",
    "\n",
    "features_CAR_TYPE = train_nomiss.groupby(['CAR_TYPE'])['TARGET_FLAG'].mean()\n",
    "print(features_CAR_TYPE)\n",
    "features_cat.CAR_TYPE=features_cat.CAR_TYPE.replace('Minivan',features_CAR_TYPE['Minivan'])\n",
    "features_cat.CAR_TYPE=features_cat.CAR_TYPE.replace('Panel Truck',features_CAR_TYPE['Panel Truck'])\n",
    "features_cat.CAR_TYPE=features_cat.CAR_TYPE.replace('Pickup',features_CAR_TYPE['Pickup'])\n",
    "features_cat.CAR_TYPE=features_cat.CAR_TYPE.replace('Sports Car',features_CAR_TYPE['Sports Car'])\n",
    "features_cat.CAR_TYPE=features_cat.CAR_TYPE.replace('Van',features_CAR_TYPE['Van'])\n",
    "features_cat.CAR_TYPE=features_cat.CAR_TYPE.replace('z_SUV',features_CAR_TYPE['z_SUV'])\n",
    "\n",
    "\n",
    "\n",
    "features_EDUCATION = train_nomiss.groupby(['EDUCATION'])['TARGET_FLAG'].mean()\n",
    "print(features_EDUCATION)\n",
    "features_cat.EDUCATION=features_cat.EDUCATION.replace('<High School',features_EDUCATION['<High School'])\n",
    "features_cat.EDUCATION=features_cat.EDUCATION.replace('z_High School',features_EDUCATION['z_High School'])\n",
    "features_cat.EDUCATION=features_cat.EDUCATION.replace('Bachelors',features_EDUCATION['Bachelors'])\n",
    "features_cat.EDUCATION=features_cat.EDUCATION.replace('Masters',features_EDUCATION['Masters'])\n",
    "features_cat.EDUCATION=features_cat.EDUCATION.replace('PhD',features_EDUCATION['PhD'])\n",
    "\n",
    "\n",
    "features_JOB = train_nomiss.groupby(['JOB'])['TARGET_FLAG'].mean()\n",
    "print(features_JOB)\n",
    "features_cat.JOB=features_cat.JOB.replace('Student',features_JOB['Student'])\n",
    "features_cat.JOB=features_cat.JOB.replace('z_Blue Collar',features_JOB['z_Blue Collar'])\n",
    "features_cat.JOB=features_cat.JOB.replace('Clerical',features_JOB['Clerical'])\n",
    "features_cat.JOB=features_cat.JOB.replace('Home Maker',features_JOB['Home Maker'])\n",
    "features_cat.JOB=features_cat.JOB.replace('Professional',features_JOB['Professional'])\n",
    "features_cat.JOB=features_cat.JOB.replace('Lawyer',features_JOB['Lawyer'])\n",
    "features_cat.JOB=features_cat.JOB.replace('Doctor',features_JOB['Doctor'])\n",
    "features_cat.JOB=features_cat.JOB.replace('Manager',features_JOB['Manager'])\n",
    "\n",
    "features_cat_3 = np.array(features_cat.loc[:,['CAR_TYPE','EDUCATION','JOB']]).astype(float)\n",
    "feat_name_list.append(['CAR_TYPE','EDUCATION','JOB'])\n",
    "\n",
    "features_merge_full=np.concatenate((features_merge,features_cat_3), axis=1)\n",
    "print(features_merge_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eb15f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We note that these addiational categories have some\n",
    "# differences in their positive rates.\n",
    "\n",
    "# Now that we have the data arranged let's divide it into a training and a testing dataset:\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_merge_full, target, test_size=0.30, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d754516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training set:\n",
      "--------------------------------:\n",
      "True negative:  2882\n",
      "False negative:  627\n",
      "False positve:  225\n",
      "True positive:  497\n",
      "------------------------------\n",
      "Hit rate:  0.7986291656818719\n",
      "Pecision:  0.6883656509695291\n",
      "Recall:  0.44217081850533807\n",
      "Fraction of positive predictions 0.17064523753249822\n",
      "   \n",
      "Performance on test set:\n",
      "--------------------------------:\n",
      "True negative:  1219\n",
      "False negative:  280\n",
      "False positve:  117\n",
      "True positive:  198\n",
      "------------------------------\n",
      "Hit rate:  0.7811466372657111\n",
      "Pecision:  0.6285714285714286\n",
      "Recall:  0.41422594142259417\n",
      "Fraction of positive predictions 0.17364939360529216\n",
      "   \n",
      "------------------------------\n",
      "Weights of features:\n",
      "['AGE', 'HOMEKIDS', 'YOJ', 'INCOME', 'HOME_VAL', 'TRAVTIME', 'BLUEBOOK', 'TIF', 'OLDCLAIM', 'CLM_FREQ', 'MVR_PTS', 'CAR_AGE', ['KIDSDRIV', 'PARENT1', 'MSTATUS', 'SEX', 'CAR_USE', 'RED_CAR', 'REVOKED', 'URBANICITY'], ['CAR_TYPE', 'EDUCATION', 'JOB']]\n",
      "[[-0.00669893  0.04484545 -0.02993572 -0.10157288 -0.22780225  0.2517891\n",
      "  -0.20077008 -0.23608395 -0.11355018  0.23758488  0.25290757 -0.01043563\n",
      "   0.11944273  0.14723591 -0.21470227  0.0431204   0.29054904 -0.09160396\n",
      "   0.27693849  0.92129028  0.33433336  0.15462713  0.23751988]]\n"
     ]
    }
   ],
   "source": [
    "# Let's try to make a first prediction using logistic regression \n",
    "# which is among the most popular machine machine learning techniques\n",
    "# and often gives satisfactory performance.\n",
    "\n",
    "# First let's rescale our features using\n",
    "# the training data set.\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "#Let's train the model and see how it does on the training set first.\n",
    "LR = LogisticRegression(random_state=0).fit(X_train,y_train)\n",
    "y_train_pred=LR.predict(X_train)\n",
    "y_test_pred=LR.predict(X_test)\n",
    "#print(target_prediction.shape)\n",
    "\n",
    "# To evaluate the model I will look at the number of occurence of\n",
    "# True Negatives, False Negative, False Positives and True Positives,\n",
    "# as well as at a few quantities derived from these (Hit rate, Recall \n",
    "# and Precision). This will allow me to assess how often we predict\n",
    "# a certain outcome and how often \n",
    "\n",
    "def evaluation(B_train,B_train_pred,B_test,B_test_pred):\n",
    "    Confusion = confusion_matrix(B_train,B_train_pred)\n",
    "    print('Performance on training set:')\n",
    "    print('--------------------------------:')\n",
    "    print('True negative: ', Confusion[0,0])\n",
    "    print('False negative: ', Confusion[1,0])\n",
    "    print('False positve: ', Confusion[0,1])\n",
    "    print('True positive: ', Confusion[1,1])\n",
    "    print('------------------------------')\n",
    "    print('Hit rate: ', (Confusion[0,0]+Confusion[1,1])/(Confusion[0,0]+Confusion[1,1]+Confusion[0,1]+Confusion[1,0]))\n",
    "    print('Pecision: ',Confusion[1,1]/(Confusion[1,1]+Confusion[0,1]))\n",
    "    print('Recall: ',Confusion[1,1]/(Confusion[1,1]+Confusion[1,0]))\n",
    "    print('Fraction of positive predictions',\n",
    "          (Confusion[1,1]+Confusion[0,1])/(Confusion[0,0]+Confusion[1,1]+Confusion[0,1]+Confusion[1,0]))\n",
    "    #(target)\n",
    "    print('   ')\n",
    "    Confusion = confusion_matrix(B_test,B_test_pred)\n",
    "    print('Performance on test set:')\n",
    "    print('--------------------------------:')\n",
    "    print('True negative: ', Confusion[0,0])\n",
    "    print('False negative: ', Confusion[1,0])\n",
    "    print('False positve: ', Confusion[0,1])\n",
    "    print('True positive: ', Confusion[1,1])\n",
    "    print('------------------------------')\n",
    "    print('Hit rate: ', (Confusion[0,0]+Confusion[1,1])/(Confusion[0,0]+Confusion[1,1]+Confusion[0,1]+Confusion[1,0]))\n",
    "    print('Pecision: ',Confusion[1,1]/(Confusion[1,1]+Confusion[0,1]))\n",
    "    print('Recall: ',Confusion[1,1]/(Confusion[1,1]+Confusion[1,0]))\n",
    "    print('Fraction of positive predictions',\n",
    "          (Confusion[1,1]+Confusion[0,1])/(Confusion[0,0]+Confusion[1,1]+Confusion[0,1]+Confusion[1,0]))\n",
    "    print('   ')\n",
    "    print('------------------------------')\n",
    "    \n",
    "evaluation(y_train,y_train_pred,y_test,y_test_pred)\n",
    "\n",
    "print('Weights of features:')\n",
    "print(feat_name_list)\n",
    "print(LR.coef_)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5bd5ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training set:\n",
      "--------------------------------:\n",
      "True negative:  2966\n",
      "False negative:  510\n",
      "False positve:  141\n",
      "True positive:  614\n",
      "------------------------------\n",
      "Hit rate:  0.8461356653273457\n",
      "Pecision:  0.8132450331125828\n",
      "Recall:  0.5462633451957295\n",
      "Fraction of positive predictions 0.17844481210115812\n",
      "   \n",
      "Performance on test set:\n",
      "--------------------------------:\n",
      "True negative:  1227\n",
      "False negative:  281\n",
      "False positve:  109\n",
      "True positive:  197\n",
      "------------------------------\n",
      "Hit rate:  0.7850055126791621\n",
      "Pecision:  0.6437908496732027\n",
      "Recall:  0.4121338912133891\n",
      "Fraction of positive predictions 0.16868798235942667\n",
      "   \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The performance is about similar on the training set and on the\n",
    "# test set. There is certainly some skill, but the overall performance\n",
    "# could be better. In particular, the model strongly underestimates \n",
    "# positive events. If the variable that we would like to predict is\n",
    "# indeed that damage has occured, an insurance company would not want\n",
    "# to underestimate this.\n",
    "# Looking at the coefficients (weights), I find interesting that the Urbanicity\n",
    "# gets such a large weight. Since we rescaled the values before training\n",
    "# comparing the coefficients can be an indication of the influence of parameters\n",
    "# although one needs to be careful, as the range of possible feature values\n",
    "# can still vary somewhat, since the rescaling occured using the variance.\n",
    "\n",
    "# So let us try to do a bit better on the positive cases. An idea to\n",
    "# train the model towards more readily predicting damage (positive cases) is to choose a\n",
    "# cost function that adds a higher cost to false negative than to false\n",
    "# positive classifications. For this purpose we will choose a suport vector\n",
    "# machine (SVM), in which we can adjust the class weight.\n",
    "\n",
    "# Let's first see how the SVM does without balanced weights. The default svm\n",
    "# here uses a gaussian-like kernel.\n",
    "\n",
    "Tsvm = svm.SVC()\n",
    "#Tsvm = svm.SVC(class_weight='balanced')\n",
    "#Tsvm.fit(X_test, y_test)\n",
    "\n",
    "\n",
    "svm_tr = Tsvm.fit(X_train,y_train)\n",
    "y_train_pred=svm_tr.predict(X_train)\n",
    "y_test_pred=svm_tr.predict(X_test)\n",
    "#print(target_prediction.shape)\n",
    "\n",
    "evaluation(y_train,y_train_pred,y_test,y_test_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43abfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on training set:\n",
      "--------------------------------:\n",
      "True negative:  2478\n",
      "False negative:  169\n",
      "False positve:  629\n",
      "True positive:  955\n",
      "------------------------------\n",
      "Hit rate:  0.8113921058851336\n",
      "Pecision:  0.6029040404040404\n",
      "Recall:  0.849644128113879\n",
      "Fraction of positive predictions 0.3743795792956748\n",
      "   \n",
      "Performance on test set:\n",
      "--------------------------------:\n",
      "True negative:  1001\n",
      "False negative:  135\n",
      "False positve:  335\n",
      "True positive:  343\n",
      "------------------------------\n",
      "Hit rate:  0.74090407938258\n",
      "Pecision:  0.5058997050147492\n",
      "Recall:  0.7175732217573222\n",
      "Fraction of positive predictions 0.3737596471885336\n",
      "   \n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The SVN does better on the training set, but\n",
    "# is comparable to logistic regression on the\n",
    "# test set. The fact that the performance is not\n",
    "# increasing anymore in the test set despite the improvement\n",
    "# in the training set, could indicate that with our features\n",
    "# and the amount of training data we have, we will not be \n",
    "# able to do much better.\n",
    "\n",
    "# Now let's see what happens if we choose to change class weight.\n",
    "\n",
    "Tsvm = svm.SVC(class_weight='balanced')\n",
    "#Tsvm.fit(X_test, y_test)\n",
    "svmW_tr = Tsvm.fit(X_train,y_train)\n",
    "y_train_pred=svmW_tr.predict(X_train)\n",
    "y_test_pred=svmW_tr.predict(X_test)\n",
    "#print(target_prediction.shape)\n",
    "\n",
    "evaluation(y_train,y_train_pred,y_test,y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b21564a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the adjusted cost function, the resulting\n",
    "# fraction of predicted positive results is much higher,\n",
    "# at the expense of substantially losing Precision due to\n",
    "# a high rate of false positives. If avoiding false positives\n",
    "# than this could be an option. However, the overall hit rate\n",
    "# is comparable tp what it would be,\n",
    "# if we had always predicted TARGET_VALUE=0\n",
    "\n",
    "# Possible paths forward could be to try and use decision tree\n",
    "# algorithms, or to find more features or more training data.\n",
    "# For instance, I could fill unassigned values in the data I \n",
    "# discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c45bd0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features:  12\n",
      "Class features:  11\n",
      "(1612, 23)\n"
     ]
    }
   ],
   "source": [
    "# Let's finally do a few predictions.\n",
    "test = pd.read_csv('test_auto.csv')\n",
    "\n",
    "test_features=test.loc[:,'INDEX':'URBANICITY']\n",
    "\n",
    "# Now if this were prediction that were going to be used\n",
    "# I would think about what values to assign to the missing\n",
    "# values. For numerical values good missing values could be\n",
    "# the mean or the median. For categorical data, either I could\n",
    "# choose the most frequent category, or I could choose one which\n",
    "# is unlikely going to affect the prediction (if that is possible)\n",
    "# I also checked the data and saw that only a few features have missing\n",
    "# values, so an alternative strategy could have been to train the \n",
    "# models without the affected features and check performance. \n",
    "#\n",
    "# In the end, to save some time, I will just ignore the data with\n",
    "# missing values. I hope this is ok.\n",
    "\n",
    "#test_features=train_nomiss.loc[:,'KIDSDRIV':'URBANICITY']\n",
    "test_nomiss=test_features\n",
    "for column in test_features:\n",
    "    if column !='TARGET_FLAG' and column !='TARGET_AMT':\n",
    "        test_nomiss=test_nomiss[pd.notnull(test_nomiss[column])]\n",
    "\n",
    "test_nomiss.INCOME=test_nomiss.INCOME.str.replace(',','').str.extract('(\\d+)', expand=False)\n",
    "test_nomiss.HOME_VAL=test_nomiss.HOME_VAL.str.replace(',','').str.extract('(\\d+)', expand=False)\n",
    "test_nomiss.BLUEBOOK=test_nomiss.BLUEBOOK.str.replace(',','').str.extract('(\\d+)', expand=False)\n",
    "test_nomiss.OLDCLAIM=test_nomiss.OLDCLAIM .str.replace(',','').str.extract('(\\d+)', expand=False)\n",
    "\n",
    "test_nomiss_index=np.array(test_nomiss.INDEX.astype(int))\n",
    "\n",
    "test_num_list=[]\n",
    "test_cat_list=[]\n",
    "for feat in test_nomiss:\n",
    "    if test_nomiss[feat].nunique() >=6 and feat !='JOB' and feat!='EDUCATION' and feat!='CAR_TYPE' and feat !='INDEX':\n",
    "        test_num_list.append(feat)\n",
    "    else:\n",
    "        if feat !='INDEX':\n",
    "            test_cat_list.append(feat)\n",
    "        \n",
    "test_features_num=np.array(test_nomiss.loc[:,num_list].astype(float))\n",
    "print(\"Numerical features: \", test_features_num.shape[1])\n",
    "\n",
    "test_features_cat=test_nomiss.loc[:,cat_list]\n",
    "print(\"Class features: \", test_features_cat.shape[1])\n",
    "\n",
    "\n",
    "#This step could be automated if it was to be repeated regularly:\n",
    "test_features_cat.PARENT1=test_features_cat.PARENT1.replace('No','0').replace('Yes','1')\n",
    "test_features_cat.MSTATUS=test_features_cat.MSTATUS.replace('z_No','0').replace('Yes','1')\n",
    "test_features_cat.SEX=test_features_cat.SEX.replace('z_F','0').replace('M','1')\n",
    "test_features_cat.CAR_USE=test_features_cat.CAR_USE.replace('Private','0').replace('Commercial','1')\n",
    "test_features_cat.RED_CAR=test_features_cat.RED_CAR.replace('no','0').replace('yes','1')\n",
    "test_features_cat.REVOKED=test_features_cat.REVOKED.replace('No','0').replace('Yes','1')\n",
    "test_features_cat.URBANICITY=test_features_cat.URBANICITY.replace('z_Highly Rural/ Rural','0').replace('Highly Urban/ Urban','1')\n",
    "\n",
    "\n",
    "test_features_cat_2=np.array(test_features_cat.loc[:,['KIDSDRIV','PARENT1','MSTATUS','SEX',\n",
    "                                            'CAR_USE','RED_CAR','REVOKED','URBANICITY']]).astype(float)\n",
    "\n",
    "test_features_merge=np.concatenate((test_features_num, test_features_cat_2), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Again: This step could be automated if necessary. Be careful here, as we have to take the\n",
    "# mean values of the training set.\n",
    "test_features_cat.CAR_TYPE=test_features_cat.CAR_TYPE.replace('Minivan',features_CAR_TYPE['Minivan'])\n",
    "test_features_cat.CAR_TYPE=test_features_cat.CAR_TYPE.replace('Panel Truck',features_CAR_TYPE['Panel Truck'])\n",
    "test_features_cat.CAR_TYPE=test_features_cat.CAR_TYPE.replace('Pickup',features_CAR_TYPE['Pickup'])\n",
    "test_features_cat.CAR_TYPE=test_features_cat.CAR_TYPE.replace('Sports Car',features_CAR_TYPE['Sports Car'])\n",
    "test_features_cat.CAR_TYPE=test_features_cat.CAR_TYPE.replace('Van',features_CAR_TYPE['Van'])\n",
    "test_features_cat.CAR_TYPE=test_features_cat.CAR_TYPE.replace('z_SUV',features_CAR_TYPE['z_SUV'])\n",
    "\n",
    "test_features_cat.EDUCATION=test_features_cat.EDUCATION.replace('<High School',features_EDUCATION['<High School'])\n",
    "test_features_cat.EDUCATION=test_features_cat.EDUCATION.replace('z_High School',features_EDUCATION['z_High School'])\n",
    "test_features_cat.EDUCATION=test_features_cat.EDUCATION.replace('Bachelors',features_EDUCATION['Bachelors'])\n",
    "test_features_cat.EDUCATION=test_features_cat.EDUCATION.replace('Masters',features_EDUCATION['Masters'])\n",
    "test_features_cat.EDUCATION=test_features_cat.EDUCATION.replace('PhD',features_EDUCATION['PhD'])\n",
    "\n",
    "test_features_cat.JOB=test_features_cat.JOB.replace('Student',features_JOB['Student'])\n",
    "test_features_cat.JOB=test_features_cat.JOB.replace('z_Blue Collar',features_JOB['z_Blue Collar'])\n",
    "test_features_cat.JOB=test_features_cat.JOB.replace('Clerical',features_JOB['Clerical'])\n",
    "test_features_cat.JOB=test_features_cat.JOB.replace('Home Maker',features_JOB['Home Maker'])\n",
    "test_features_cat.JOB=test_features_cat.JOB.replace('Professional',features_JOB['Professional'])\n",
    "test_features_cat.JOB=test_features_cat.JOB.replace('Lawyer',features_JOB['Lawyer'])\n",
    "test_features_cat.JOB=test_features_cat.JOB.replace('Doctor',features_JOB['Doctor'])\n",
    "test_features_cat.JOB=test_features_cat.JOB.replace('Manager',features_JOB['Manager'])\n",
    "\n",
    "test_features_cat_3 = np.array(test_features_cat.loc[:,['CAR_TYPE','EDUCATION','JOB']]).astype(float)\n",
    "\n",
    "test_features_merge_full=np.concatenate((test_features_merge,test_features_cat_3), axis=1)\n",
    "\n",
    "        \n",
    "\n",
    "print(test_features_merge_full.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06e5b2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of positive predictions for SVM without adjustment:  0.16377171215880892\n",
      "Fraction of positive predictions for SVM with adjustment:  0.3939205955334988\n"
     ]
    }
   ],
   "source": [
    "# Now that we have processed that data and put it in the\n",
    "# same order as for the training examples, we just need to\n",
    "# do the feature scaling and we are good to go:\n",
    "X_pred = scaler.transform(test_features_merge_full)\n",
    "\n",
    "# Let's do the predictions with the SVM with and\n",
    "# without adjusted weights\n",
    "\n",
    "y_pred_svm=svm_tr.predict(X_pred)\n",
    "y_pred_svmW=svmW_tr.predict(X_pred)\n",
    "\n",
    "print('Fraction of positive predictions for SVM without adjustment: ',np.mean(y_pred_svm))\n",
    "print('Fraction of positive predictions for SVM with adjustment: ',np.mean(y_pred_svmW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5298d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows what we could expect: The SVM with adjustment \n",
    "# predicts more often positive cases. The value of positive\n",
    "# cases was about 26% (see above), so the SVM without adjustment\n",
    "# tends to underestimate positive cases and the SVM with adjustment\n",
    "# tends to overestimate positive cases.\n",
    "# We could now try to go back to the training and the test set and try\n",
    "# to adjust the weights further. But in that case we would need to divide\n",
    "# the test set, into a validation and a cross-validation test as we would\n",
    "# use the weight parameter to try and fit the validation set.\n",
    "\n",
    "# Now let's prepare the data to write it to the output.\n",
    "# The selection of the categorical data retained the original\n",
    "# indexing, so despite dropping cases with missing values\n",
    "# the different cases are still identifiable.\n",
    "y_pred_con=np.stack((test_nomiss_index.T,y_pred_svm.T,y_pred_svmW.T), axis=1)\n",
    "Prediction=pd.DataFrame(y_pred_con,index=test_features_cat.index,\n",
    "                        columns=['INDEX','TARGET_FLAG_SVM','TARGET_FLAG_SVMW'])\n",
    "\n",
    "Prediction.to_csv('Insurance_prediction.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
